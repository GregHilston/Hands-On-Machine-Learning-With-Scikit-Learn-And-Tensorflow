{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#  Real Estate Valuation\n",
    "\n",
    "This is a real estate multivariate regression problem. Well be going through the \"checklist\" defined in Appendix B in the book [Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291). I'll be expliclity answering all the questions and going through each step to improve my learning, even though a lot of the questions don't make sense or apply super well to just doing this process for learning.\n",
    "\n",
    "I'll preface answers with\n",
    "\n",
    "> Pretend\n",
    "\n",
    "if I'm making up an answer for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# allows our matplotlib graphs to be display inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request # for fetching our raw data from the web\n",
    "import pandas as pd # for easily manipulating our data\n",
    "from pandas.plotting import scatter_matrix # for comparing all independent and dependent variables against each other\n",
    "import seaborn as sns # for pretty graphs\n",
    "import matplotlib.pyplot as plt # to stop graphs from plotting over one another\n",
    "from scipy.stats import shapiro # for testing for normality\n",
    "import statsmodels.api as sm # for making QQ plots\n",
    "from IPython.display import Image, display # for display a local image, as the markdown way does not work with sibling folders\n",
    "from sklearn.preprocessing import RobustScaler # for scaling our numerical independent variables\n",
    "from sklearn.model_selection import train_test_split # for train test split\n",
    "from sklearn.linear_model import LinearRegression # for linear regression\n",
    "from sklearn.ensemble import RandomForestRegressor # for random forest regressor\n",
    "from sklearn.metrics import mean_absolute_error # for MAE\n",
    "from keras.models import Sequential # for neural network\n",
    "from keras.wrappers.scikit_learn import KerasRegressor # for keras regressor\n",
    "from keras.layers import Dense # for layers\n",
    "from sklearn.model_selection import cross_val_score, KFold # for crossvalidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Part A: Frame the Problem and Look at the Big Picture\n",
    " \n",
    "1. Define the objective in business terms.\n",
    "\n",
    "(Pretend) We want to predict real estate valuations in New Taipei City, Taiwan to increase our ability to effectively bid on lots.\n",
    "\n",
    "2. How will your solution be used?\n",
    "\n",
    "(Pretend) Our solution will be used to ensure we our bids are accurate, maximizing profit.\n",
    "\n",
    "3. What are the current solutions/workarounds (if any)?\n",
    "\n",
    "(Pretend) None.\n",
    "\n",
    "4. How should you frame this problem (supervised/unsupervised, online/offline, etc.)?\n",
    "\n",
    "Supervised. The data sets have house_price_of_unit_area attached. \n",
    "\n",
    "Offline. All the data that existed has been already gathered and all the training will be done at once.\n",
    "\n",
    "5. How should performance be measured?\n",
    "\n",
    "When deciding which performance metric to use, we have a lot of options to pick from.\n",
    "\n",
    "1. Mean Squared Error (MSE)\n",
    "2. Root Mean Squared Error (RMSE)\n",
    "3. Mean Absolute Error (MAE)\n",
    "4. R Squared (R²)\n",
    "5. Adjusted R Squared $(R²)$\n",
    "6. Mean Square Percentage Error (MSPE)\n",
    "7. Mean Absolute Percentage Error (MAPE)\n",
    "8. Root Mean Squared Logarithmic Error (RMSLE)\n",
    "\n",
    "I have decided that we'll use MAE:\n",
    "\n",
    "$\\text{MAE} = \\frac{1}{N}\\sum_{i=1}^{N}|{y_{i}-\\hat{y}_{i}}|$\n",
    "\n",
    "I've decided to use MAE, as I believe we'll have some outliers in our data set. \n",
    "\n",
    "I used this [article](https://towardsdatascience.com/how-to-select-the-right-evaluation-metric-for-machine-learning-models-part-1-regrression-metrics-3606e25beae0) to help decide what would be a proper performance measurement.\n",
    "\n",
    "6. Is the performance measure aligned with the business objective?\n",
    "\n",
    "Yes, since we're predicting the value of houses, we'll be able to look at our error function's output and compare different models against each other, knowing we can treat a 10 MSE as exactly twice as bad as a 5 MSE. This works very well with the financial aspect of what we're modeling here.\n",
    "\n",
    "7. What would be the minimum performance needed to reach the business objective?\n",
    "\n",
    "(Pretend) We want to be within 25% of the valuation of a house, so we can see if it is worth gathering more data to try and further refine our model.\n",
    "\n",
    "8. What are comparable problems? Can you reuse experience or tools?\n",
    "\n",
    "(Pretend) Our company has had no comparable problems, nor can we reuse experience or tools.\n",
    "\n",
    "9. Is human expertise available?\n",
    "\n",
    "(Pretend) No.\n",
    "\n",
    "10. How would you solve the problem manually?\n",
    "\n",
    "I'd attempt to solve this problem manually by looking at all the houses that sold for the most amount and the houses that sold for the least amount and look for patterns that may elude to what would effect each house's price.\n",
    "\n",
    "11. List the assumptions you (or others) have made so far.\n",
    "\n",
    "While I have no assumptions regarding the dataset as a whole, I'll be listing column specific assumptions in section C.\n",
    "\n",
    "12. Verify assumptions if possible.\n",
    "\n",
    "Will do in Part C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Part B: Get the Data\n",
    "\n",
    "_Note: automate as much as possible so you can easily get fresh data._\n",
    "\n",
    "1. List the data you need and how much you need.\n",
    "\n",
    "We'll just use the dataset we found before starting this project. If we were to answer this without acknowledging the dataset we've already found, we'd want a many rows as possible where columns would be potential variables that contributed to valuation of a house.\n",
    "\n",
    "2. Find and document where you can get that data.\n",
    "\n",
    "We can get this data at this [location](https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set).\n",
    "\n",
    "3. Check how much space it will take.\n",
    "\n",
    "32 KB.\n",
    "\n",
    "4. Check legal obligations, and get authorization if necessary.\n",
    "\n",
    "UCI's data is free.\n",
    "\n",
    "5. Get access authorizations.\n",
    "\n",
    "Not needed.\n",
    "\n",
    "6. Create a workspace (with enough storage space).\n",
    "\n",
    "Done using [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/).\n",
    "\n",
    "7. Get the data.\n",
    "\n",
    "Done below.\n",
    "\n",
    "8. Convert the data to a format you can easily manipulate (without changing the data itself).\n",
    "\n",
    "Not needed.\n",
    "\n",
    "9. Ensure sensitive information is deleted or protected (e.g., anonymized).\n",
    "\n",
    "Not needed.\n",
    "\n",
    "10. Check the size and type of data (time series, sample, geographical, etc.).\n",
    "\n",
    "I'm not sure what to call the kind of data we're working with. Each row represents the action of selling a house, how much it sold for and a few other potentially related measurements.\n",
    "\n",
    "11. Sample a test set, put it aside, and never look at it (no data snooping!).\n",
    "\n",
    "We'll do this later, as we have cleaning to perform in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# We we should store our raw hosuing data\n",
    "STORE_RAW_HOUSING_DATA_DESTINATION_PATH = \"../data/raw/real_estate_valuation_data_set.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def fetch_raw_housing_data(STORE_RAW_HOUSING_DATA_DESTINATION_PATH: str = STORE_RAW_HOUSING_DATA_DESTINATION_PATH):\n",
    "    \"\"\"Fetches our raw housing data.\n",
    "    \n",
    "    \"\"\"\n",
    "    # We we can fetch out raw data from\n",
    "    RAW_DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00477/Real%20estate%20valuation%20data%20set.xlsx\"\n",
    "    \n",
    "    urllib.request.urlretrieve(RAW_DATA_URL, STORE_RAW_HOUSING_DATA_DESTINATION_PATH)\n",
    "    \n",
    "fetch_raw_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_raw_hosuing_data(STORE_RAW_HOUSING_DATA_DESTINATION_PATH: str = STORE_RAW_HOUSING_DATA_DESTINATION_PATH) -> pd.DataFrame:\n",
    "    \"\"\"Loads the raw housing data we've previously fetched\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    names = [\"No\", \"transaction_date\", \"house_age\", \"distance_to_the_nearest_MRT_station\", \"number_of_convenience_stores\", \"latitude\", \"longitude\", \"house_price_of_unit_area\"]\n",
    "        \n",
    "    # creating our base data frame\n",
    "    # I know there needs to be some cleaning on the `transaction_date` independent variable.\n",
    "    # We could have done that here using the parse_dates keyword flag and a parser keyword flag to a lambda or function, but we'll handle that later\n",
    "    df = pd.read_excel(STORE_RAW_HOUSING_DATA_DESTINATION_PATH, names=names, index_col=\"No\")\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_raw_hosuing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We have successfully gathered the data we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Part C: Explore the Data\n",
    "\n",
    "_Note: try to get insights from a field expert for these steps._\n",
    "\n",
    "1. Create a copy of the data for exploration (sampling it down to a manageable size if necessary).\n",
    "\n",
    "The data is not big enough for me to consider this necessary. \n",
    "\n",
    "2. Create a Jupyter notebook to keep a record of your data exploration.\n",
    "\n",
    "We'll be doing that in this notebook.\n",
    "\n",
    "3. Study each attribute and its characteristics:\n",
    "\n",
    "  - Name\n",
    "  - Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\n",
    "  - % of missing values\n",
    "  - Noisiness and type of noise (stochastic, outliers, rounding errors, etc.)\n",
    "  - Possibly useful for the task?\n",
    "  - Type of distribution (Gaussian, uniform, logarithmic, etc.)\n",
    "  \n",
    "Will do on a column by column basis. Skipping noiseness analysis and type of distribution. I rolled \"Possibly useful\" into a new bullet I called \"assumptions\".\n",
    "\n",
    "4. For supervised learning tasks, identify the target attribute(s).\n",
    "\n",
    "The taret attribute is `house_price_of_unit_area`\n",
    "\n",
    "5. Visualize the data.\n",
    "\n",
    "We'll do this column by column and then a big scatter matrix.\n",
    "\n",
    "6. Study the correlations between attributes.\n",
    "\n",
    "We'll evaluate the correlations visually and by using Pearson's R.\n",
    "\n",
    "7. Study how you would solve the problem manually.\n",
    "\n",
    "I believe we went over this prior, but I would solve the problem manually by looking at the extremes of the data. I'd look at the most expensive houses and the least expensive houses and attempt to explain why those houses were cheaper or more expensive. Just to be clear, I would not want to look at outliers specifically but rather the non-outliers on each end of the valuation.\n",
    "\n",
    "8. Identify the promising transformations you may want to apply.\n",
    "\n",
    "I plan to clean `transaction_date`, so its easier to query. I'm not exactly sure what format I'll use, but whatever Pandas suggests.\n",
    "\n",
    "9. Identify extra data that would be useful (go back to “Get the Data”).\n",
    "\n",
    "It would be useful to get:\n",
    "\n",
    "- number of bathrooms\n",
    "- number of bedrooms\n",
    "- is there an attic\n",
    "- is there a basement\n",
    "- is there a garage\n",
    "- amount of land\n",
    "- crime in the area\n",
    "- quality of education in the area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Map of the location our data is sampled from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "display(Image(filename=\"../references/city_map.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## All Columns\n",
    "\n",
    "First we'll do a very quick high level overview of all the columns together, before diving into each column individually. In a large data set, we'd use this section to help decide which columns to look at individually. Since this dataset is rather small, I'll take the oppurtunity to explore all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Matrix\n",
    "\n",
    "We'll create single graphic of all columns against all other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "scatter_matrix(df, alpha=0.6, figsize=(30, 30), diagonal='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "I like to look at the scatter matrix when we don't have a lot of columns, just to look at the big picture in one big graph. This generally isn't the best way to check for correlation between two columns, as its a visual check and we already looked at each of the independent columns against the dependent column already. Instead we'll be using Pearson's R, also know as the standard correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "corr_matrix[\"house_price_of_unit_area\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The correlation coefficients range from $-1$ to $1$. Close to $1$ tells us there's a strong positive correlation, while close to $-1$ means there's a strong negative correlation, and close to $0$ means no correlation.\n",
    "\n",
    "We see here that `number_of_convenience_stores`, `latitude` and `longitude` look to have a strong positive correlation, while I'd say `transaction-date` has no correlation.\n",
    "\n",
    "`house_age` has a weak negative correlation and `distance_to_the_nearest_MRT_station` has a strong negative correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "top_corr_features = corr_matrix.index\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "g=sns.heatmap(df[top_corr_features].corr(), annot=True, cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This visualization makes it easy to just look at the row of `house_price_of_unit_area`, and use the color coordination to get a quick idea of the correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Visualize Data Geographically\n",
    "\n",
    "I believe it'd be very useful if I could superimpose this graph with a graphic of the location of the data. Additionally, if the dots were colored on a gradient based on their `house_price_of_unit_area`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We'll address each column, starting with the information the [website](https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set) we got the data from and then moving into any observations we have ourselves regarding the data. Then we'll use the `describe` function to get a general feel for the data, along with a violin plot and followed by performing the Shapiro-Wilk test to test for normality. This test was chosen as we have a small number of samples, which is generally a perferred prerequisite for this test. \n",
    "\n",
    "Additionally we're going to look for outliers across every column. To do this, I was going to leverage the standard score, also known as the z-score. This score is a measurement of how many standard deviations away from the mean. Unfortunately this score does not make much sense when the data you're working with is not gaussian, which is our case. \n",
    "\n",
    "With that in mind, I'll be leveraging a box-and-whisker plot to visualize outliers and use some metric like\n",
    "\n",
    "> An outlier is any value that's $1.5$ times above or below the interquartile range\n",
    "\n",
    "_Note: The assumptions were made prior to running the code._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def violin_plot(series: pd.Series):\n",
    "    \"\"\"Displays a violin plot for a single Series\n",
    "    \n",
    "    \"\"\"\n",
    "    sns.violinplot(series).set_title(f\"Violin Plot of column {series.name}\")\n",
    "    plt.figure() # ensures this graph does not plot over another graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_missing(series: pd.Series):\n",
    "    \"\"\"Displays how many rows are missing for a single Series\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"{series.isna().sum()} missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def box_plot(series: pd.Series):\n",
    "    \"\"\"Displays a Boxplot for a single Series\n",
    "    \n",
    "    \"\"\"\n",
    "    sns.boxplot(x = series).set_title(f\"Boxplot of column {series.name}\")\n",
    "    plt.figure() # ensures this graph does not plot over another graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def outliers_via_iqr(series: pd.Series):\n",
    "    \"\"\"Displays number of outliers using the IQR, for a single Series\n",
    "    \n",
    "    \"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    print(f\"IQR: {IQR}\")\n",
    "    \n",
    "    outliers = list(filter(lambda row: (row < (Q1 - 1.5 * IQR)) | (row > (Q3 + 1.5 * IQR)), series))\n",
    "    print(f\"found {len(outliers)} outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_outliers(series: pd.Series):\n",
    "    \"\"\"Displays how many outliers for a given Series, visually and mathmatically.\n",
    "    \"\"\"\n",
    "    box_plot(series)\n",
    "    outliers_via_iqr(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def qq_plot(series: pd.Series):\n",
    "    \"\"\"Displays a QQ plot for a single Series\n",
    "    \n",
    "    \"\"\"\n",
    "     # visual normality test\n",
    "    ax = sm.qqplot(series, line='45')\n",
    "    ax.suptitle(f\"Q-Q Plot of column {series.name}\")\n",
    "    plt.figure() # ensures this graph does not plot over another graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def shapiro_wilk(series: pd.Series, alpha: int = 0.05):\n",
    "    \"\"\"Performs the Shapiro-Wilk test, a mathmatical test for normality.\n",
    "    This test is specifically interested in the tails of a distribution and \n",
    "    should not be used with large datasets.\n",
    "    \n",
    "    \"\"\"\n",
    "    stat, p = shapiro(series)\n",
    "    \n",
    "    print(f\"\\nShapiro-Wilk stat {stat} p {p}\")\n",
    "    \n",
    "    if p > alpha: \n",
    "        print('Sample looks Gaussian (fail to reject H0)')\n",
    "    else:\n",
    "        print('Sample does not look Gaussian (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def test_for_normality(series:pd.Series):\n",
    "    \"\"\"Performs a series of tests for normality. Both visual and mathmatical.\n",
    "    \n",
    "    \"\"\"\n",
    "    qq_plot(series)\n",
    "    shapiro_wilk(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def scatter_plot(x: pd.Series, y: pd.Series):\n",
    "    \"\"\"Plots a scatter plot given an X and Y series\n",
    "    \n",
    "    \"\"\"\n",
    "    sns.scatterplot(x=x, y=y, alpha=0.8).set_title(f\"Scatter Plot of {x.name} against {y.name}\")\n",
    "    plt.figure() # ensures this graph does not plot over another graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def regression_line(x: pd.Series, y: pd.Series):\n",
    "    \"\"\"Plots a regression line, showing a linear elationship between \n",
    "    X and Y, if any.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #sns.regplot(x=x, y=y).set_title(f\"Regression Line Plot of {x.name} against {y.name}\") # a dufferent regression style\n",
    "    sns.jointplot(x=x, y=y, kind=\"regg\")\n",
    "    plt.figure() # ensures this graph does not plot over another graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def high_level_overview(independent: pd.Series, dependent: pd.Series = None):\n",
    "    \"\"\"Gives a high level overview of a single Pandas column.\n",
    "    \n",
    "    \"\"\"\n",
    "    print(independent.describe())\n",
    "    violin_plot(independent)\n",
    "    evaluate_missing(independent)\n",
    "    evaluate_outliers(independent)\n",
    "    test_for_normality(independent)\n",
    "    \n",
    "    if dependent is not None:\n",
    "        scatter_plot(independent, dependent)\n",
    "        regression_line(independent, dependent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Independent Variable `transaction_date`\n",
    "\n",
    "> the transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "high_level_overview(df[\"transaction_date\"], df[\"house_price_of_unit_area\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\n",
    "  - date\n",
    "- Assumptions\n",
    "  - house_price_of_unit_area and transaction date will have a positive correlation. \n",
    "    - I believe as time has gone on, the value of all houses has gone up. This might be due to a bigger population, so demand has gone up and supply has not. I'm drawing from what I know in general in the US, which may not apply to Taiwan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Independent Variable `house_age`\n",
    "\n",
    "> the house age (unit: year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "high_level_overview(df[\"house_age\"], df[\"house_price_of_unit_area\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\n",
    "  - float\n",
    "- Assumptions\n",
    "  - `house_age` and `house_price_of_unit_area` will have a negative correlation\n",
    "    - I believe that newer houses will be worth more. Again, my assumptions are based on what I know of the US. In the US, houses have gotten bigger throughout time. I imagine this trend might exist elsewhere in the world as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Independent Variable `distance_to_the_nearest_MRT_station`\n",
    "\n",
    "> the distance to the nearest MRT station (unit: meter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Where MRT = metro rail transit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "high_level_overview(df[\"distance_to_the_nearest_MRT_station\"], df[\"house_price_of_unit_area\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\n",
    "  - float\n",
    "- Assumpions\n",
    "  - `distance_to_the_nearest_MRT_station` and `house_price_of_unit_area` will have an inverse correlation\n",
    "    - I believe the distance to a MRT station wil be a good indicator for how urban or rural a house is. Where a smaller measurement will indicate more urban, fetching a higher `house_price_of_unit_area`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Independent Variable `number_of_convenience_stores `\n",
    "\n",
    "> the number of convenience stores in the living circle on foot (integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "high_level_overview(df[\"number_of_convenience_stores\"], df[\"house_price_of_unit_area\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\n",
    "  - int\n",
    "- Assumpions\n",
    "  - `number_of_convenience_stores` and `house_price_of_unit_area` will have a positive correlation\n",
    "    - Similiar to `number_of_convenience_stores`, I believe this will be another good measurement of how urban a house is. Again, a larger measurement here will indicate more urban and therefore fetch a higher `house_price_of_unit_area`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Independent Variable `latitude`\n",
    "\n",
    "> the geographic coordinate, latitude. (unit: degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "high_level_overview(df[\"latitude\"], df[\"house_price_of_unit_area\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\n",
    "  - float\n",
    "- Assumptions\n",
    "  - `latitude` and `house_price_of_unit_area` will have a parabolic correlation\n",
    "    - Since New Taipei City has water on its East and West, I believe the smallest `latitude` and largest `latitude` values will equate to larger `house_price_of_unit_area` due to waterfront properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Independent Variable `longitude`\n",
    "\n",
    "> the geographic coordinate, longitude. (unit: degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "high_level_overview(df[\"longitude\"], df[\"house_price_of_unit_area\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "- Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\n",
    "  - float\n",
    "- Assumptions\n",
    "  - `longitude` and `house_price_of_unit_area` and will have a positive correlation\n",
    "    - New Taipei City is a city bordered by water to the north, so I believe a higher `latitude` will relate to a more coastal city, resulting in a larger `house_price_of_unit_area`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Dependent Variable `house_price_of_unit_area`\n",
    "\n",
    "> house price of unit area (10000 New Taiwan Dollar/Ping, where Ping is a local unit, 1 Ping = 3.3 meter squared) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "high_level_overview(df[\"house_price_of_unit_area\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Dependent variable.\n",
    "\n",
    "- Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\n",
    "  - float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Part D: Prepare the Data\n",
    "\n",
    "Notes:\n",
    "  - Work on copies of the data (keep the original dataset intact).\n",
    "  - Write functions for all data transformations you apply, for five reasons:\n",
    "    1. So you can easily prepare the data the next time you get a fresh dataset\n",
    "    2. So you can apply these transformations in future projects\n",
    "    3. To clean and prepare the test set\n",
    "    4. To clean and prepare new data instances once your solution is live\n",
    "    5. To make it easy to treat your preparation choices as hyperparameters\n",
    "    \n",
    "1. Data cleaning:\n",
    "  - Fix or remove outliers (optional).\n",
    "    - I have read that this is a big no no and I will not proceed with removing or \"fixing\" outliers.\n",
    "  - Fill in missing values (e.g., with zero, mean, median...) or drop their rows (or columns).\n",
    "2. Feature selection(optional):\n",
    "  - Drop the attributes that provide no useful information for the task.\n",
    "3. Feature engineering, where appropriate:\n",
    "  - Discretize continuous features.\n",
    "  - Decompose features (e.g., categorical, date/time, etc.).\n",
    "  - Add promising transformations of features (e.g., log(x), sqrt(x), x^2, etc.).\n",
    "  - Aggregate features into promising new features.\n",
    "4. Feature scaling: standardize or normalize features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First we'll start by creating a copy of the data, so we can keep the original intack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Data Cleaning\n",
    "\n",
    "#### Outliers\n",
    "\n",
    "We will not be removing any outliers or \"fixing\" them. I have no sources or proof that any of these outliers are bad data.\n",
    "\n",
    "#### Missing Values\n",
    "\n",
    "No columns have any missing data, so no data imputation techniques will be used.\n",
    "\n",
    "#### Fixing D-Types\n",
    "\n",
    "`transaction_date` has a `float64` dtype, where it could have a `datetime` dtype. We'll levage the `pandas.read_csv`'s ability to pass in a custom `parse_dates` function to write our own parser.\n",
    "\n",
    "- https://stackoverflow.com/questions/21269399/datetime-dtypes-in-pandas-read-csv\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html#pandas.to_datetime\n",
    "- https://stackoverflow.com/questions/23797491/parse-dates-in-pandas\n",
    "\n",
    "We leverage the [datetime.strptime format codes](https://docs.python.org/3.4/library/datetime.html#strftime-strptime-behavior) to make the parsing incredibly easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def month_decimal_to_index(s: str) -> int:\n",
    "    \"\"\"Converts the weird encoding deciaml that this data set had to an actual month.\n",
    "    EX: '2013.250' -> 3, for March, the third month of the year\n",
    "    \n",
    "    \"\"\"\n",
    "    DECIMAL_VALUE_PER_MONTH = 1.00/12\n",
    "    return int(float('.' + s.split('.')[1])//DECIMAL_VALUE_PER_MONTH + 1) # we reattach the '.' we split on, as it is a decimal and we want to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def weird_date_to_normalized_year_month(weird_date_format: float):\n",
    "    month_as_int = month_decimal_to_index(str(weird_date_format))\n",
    "    \n",
    "    return str(weird_date_format).split('.')[0] + '.' + str(month_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df_clean[\"transaction_date\"] = df_clean[\"transaction_date\"].apply(weird_date_to_normalized_year_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "At this point, our column is now formatted from the strange percentage of the year, to an actual month. Where $1$ represents January and $12$ represents December. We'll now use Pandas' ability to parse these into a new dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df_clean[\"transaction_date\"] = df_clean[\"transaction_date\"].apply(lambda date: pd.datetime.strptime(str(date), \"%Y.%m\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now Pandas knows our `transaction_date` is a `datetime64` dtype. \n",
    "\n",
    "Its important to note that the original data did not encode day of the month, nor time. So I assumed the 1st of each month. Perhaps their fraction had the intent to encode the day, hour, minute, second but sense they did not show it in their examples, I will not try to decode this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step we have is converting from `datetime64` to `datetime`. We do this because of [this SO post](https://stackoverflow.com/a/49758140/1983957)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"transaction_date\"] = df_clean[\"transaction_date\"].dt.to_pydatetime()\n",
    "# df_clean[\"transaction_date\"] = df_clean[\"transaction_date\"].dt.date\n",
    "# df_clean[\"transaction_date\"].astype(\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO figure out why above isn't converting the dtype to `datetime`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Feature Engineering\n",
    "\n",
    "We're going to experiment with creating our own columns based on other columns in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `urbanness`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df_clean[\"urbanness\"] = df_clean[\"number_of_convenience_stores\"] / df[\"distance_to_the_nearest_MRT_station\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now we'll look at the correlation matrix again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "corr_matrix = df_clean.corr()\n",
    "corr_matrix[\"house_price_of_unit_area\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can see that our `urbanness` column has a positive correlation with `house_price_of_unit_area`, which was what we predicted in our initial EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `transaction_year`\n",
    "\n",
    "Now that we have our `transaction_date` cleaned, we can extract the `transaction_year` easily."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_clean[\"transaction_date\"]= pd.to_datetime(df_clean[\"transaction_date\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"transaction_year\"] = df_clean[\"transaction_date\"].map(lambda x:  x.year) # trying to use https://stackoverflow.com/a/25146337/1983957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t = t.astype(datetime.datetime)\n",
    "timestring = t.strftime('%Y.%m.%d')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_clean[\"transaction_year\"] = df[\"transaction_date\"].map(lambda x: x.year) # trying to use https://stackoverflow.com/a/25146337/1983957"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(df_clean.iloc[0][\"transaction_date\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_clean.iloc[0][\"transaction_date\"].year"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_clean[\"transaction_year\"] = df_clean[\"transaction_date\"].str.extract(r\"(\\d\\d\\d\\d)\", expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `transaction_month`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Generally, models don't work particularly well when numerical independent variables are in different scales from each other. What can happen is larger scaled independent variables can overshadow in importance, the smaller scaled independent variables. So a technique we can do is to scale our numerical models, all on the same scale, to give them each the same oppurtunity to be important. We have many options for scaling our independent variables:\n",
    "\n",
    "- \"standard scaler\"\n",
    "- \"min-max scaler\"\n",
    "- \"robust scaler\"\n",
    "- \"normalizer\"\n",
    "\n",
    "While I originally planned on using the standard scaler, we've already demonstrated that our independent columns are not gaussian, so we will skip this technique. The next technique min-max scaler seems to be the most widely used technique but the problem is that our independent variables have outliers as we previously demonstrated. That brings us to our third technique, the robust scaler, which is more resistant to outliers. For that reason, we will be using the robust scaler to scale our numerical independent variables.\n",
    "\n",
    "Used [this link](http://benalexkeen.com/feature-scaling-with-scikit-learn/) to learn about scaling. [This link](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html) demonstrates what can happen by using different scalers.\n",
    "\n",
    "Before we scale, lets reobserve the information regarding our independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We'll look at this exact output above, post sacling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "X = df_clean[[\"number_of_convenience_stores\", \"latitude\", \"longitude\", \"urbanness\", \"house_age\", \"distance_to_the_nearest_MRT_station\"]] # we had to exclude our date column\n",
    "Y = df_clean[[\"house_price_of_unit_area\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "scaler = RobustScaler().fit(X)\n",
    "df_clean[[\"number_of_convenience_stores\", \"latitude\", \"longitude\", \"urbanness\", \"house_age\", \"distance_to_the_nearest_MRT_station\"]] = scaler.fit_transform(X)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Feature Selection\n",
    "\n",
    "Why do we perform feature selection? For three benefits of performing feature selection before modeling your data are:\n",
    "\n",
    "1. Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n",
    "2. Improves Accuracy: Less misleading data means modeling accuracy improves.\n",
    "3. Reduces Training Time: Less data means that algorithms train faster.\n",
    "\n",
    "Additionally, there are four different techniques for performing feature selection:\n",
    "\n",
    "\n",
    "1. Forward Selection: The procedure starts with an empty set of features [reduced set]. The best of the original features is determined and added to the reduced set. At each subsequent iteration, the best of the remaining original attributes is added to the set.\n",
    "2. Backward Elimination: The procedure starts with the full set of attributes. At each step, it removes the worst attribute remaining in the set.\n",
    "3. Combination of forward selection and backward elimination: The stepwise forward selection and backward elimination methods can be combined so that, at each step, the procedure selects the best attribute and removes the worst from among the remaining attributes.\n",
    "4. Recursive Feature elimination: Recursive feature elimination performs a greedy search to find the best performing feature subset. It iteratively creates models and determines the best or the worst performing feature at each iteration. It constructs the subsequent models with the left features until all the features are explored. It then ranks the features based on the order of their elimination. In the worst case, if a dataset contains N number of features RFE will do a greedy search for 2N combinations of features.\n",
    "\n",
    "\n",
    "\n",
    "We'll address each column and determine if any are not good predictors of the dependent variable.\n",
    "\n",
    "\n",
    "Used [this link](https://machinelearningmastery.com/feature-selection-machine-learning-python/) and [this link](https://www.datacamp.com/community/tutorials/feature-selection-python), as I don't believe the \"Hands-On Machine Learning\" book does enough on this section."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "array = df_clean.values\n",
    "x = array[:,0:7]\n",
    "y = array[:,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries first\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# Summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "\n",
    "features = fit.transform(X)\n",
    "# Summarize selected features\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Part E: Short-List Promising Models\n",
    "\n",
    "Notes:\n",
    "  - If the data is huge, you may want to sample smaller training sets so you can train many different models in a reasonable time (be aware that this penalizes complex models such as large neural nets or Random Forests).\n",
    "  - Once again, try to automate these steps as much as possible.\n",
    "  \n",
    "1. Train many quick and dirty models from different categories (e.g., linear, naive Bayes, SVM, Random Forests, neural net, etc.) using standard parameters.\n",
    "2. Measure and compare their performance. For each model, use N-fold cross-validation and compute the mean and standard deviation of the performance measure on the N folds.\n",
    "3. Analyze the most significant variables for each algorithm.\n",
    "4. Analyze the types of errors the models make. What data would a human have used to avoid these errors?\n",
    "5. Have a quick round of feature selection and engineering.\n",
    "6. Have one or two more quick iterations of the five previous steps.\n",
    "7. Short-list the top three to five most promising models, preferring models that make different types of errors.\n",
    "\n",
    "For each model, we'll use the mean absolute error (MAE) to evaluate performance. Remember, this is a non-negative floating point, where the best value is $0.0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Splitting our data set into a training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(X, Y, test_size=0.20, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression().fit(X_TRAIN, Y_TRAIN)\n",
    "Y_PRED = linear_regression.predict(X_TEST)\n",
    "print(\"Mean Absolute Error: \", mean_absolute_error(Y_TEST, Y_PRED))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Support-vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "random_forest_regressor = RandomForestRegressor(n_estimators=20, random_state=SEED).fit(X_TRAIN, Y_TRAIN)  \n",
    "Y_PRED = random_forest_regressor.predict(X_TEST)  \n",
    "print(\"Mean Absolute Error: \", mean_absolute_error(Y_TEST, Y_PRED))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, input_dim=6, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=100, verbose=False)\n",
    "kfold = KFold(n_splits=10, random_state=SEED)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "\n",
    "estimator.fit(X, y)\n",
    "Y_PRED = estimator.predict(X_TEST)\n",
    "print(\"Mean Absolute Error: \", mean_absolute_error(Y_TEST, Y_PRED))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Part F: Fine-Tune the System\n",
    "\n",
    "Notes:\n",
    "- You will want to use as much data as possible for this step, especially as you move toward the end of fine-tuning.\n",
    "- As always automate what you can.\n",
    "\n",
    "1. Fine-tune the hyperparameters using cross-validation.\n",
    "  - Treat your data transformation choices as hyperparameters, especially when you are not sure about them (e.g., should I replace missing values with zero or with the median value? Or just drop the rows?). Unless there are very few hyperparameter values to explore, prefer random search over grid search. If training is very long, you may prefer a Bayesian optimization approach (e.g., using Gaussian process priors, as described by Jasper Snoek, Hugo Larochelle, and Ryan Adams).\n",
    "\n",
    "2. Try Ensemble methods. Combining your best models will often perform better than running themindividually.\n",
    "3. Once you are confident about your final model, measure its performance on the test set to estimate the generalization error.\n",
    "\n",
    "WARNING: Don’t tweak your model after measuring the generalization error: you would just start overfitting the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Part G: Present Your Solution\n",
    "\n",
    "1. Document what you have done.\n",
    "2. Create a nice presentation. \n",
    "  - Make sure you highlight the big picture first.\n",
    "3. Explain why your solution achieves the business objective.\n",
    "4. Don’t forget to present interesting points you noticed along the way. \n",
    "  - Describe what worked and what did not. \n",
    "  - List your assumptions and your system’s limitations.\n",
    "5. Ensure your key findings are communicated through beautiful visualizations or easy-to-remember statements (e.g., “the median income is the number-one predictor of housing prices”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Part H: Launch!\n",
    "\n",
    "1. Get your solution ready for production (plug into production data inputs, write unit tests, etc.).\n",
    "2. Write monitoring code to check your system’s live performance at regular intervals and trigger alerts when it drops.\n",
    "  - Beware of slow degradation too: models tend to “rot” as data evolves.\n",
    "  - Measuring performance may require a human pipeline (e.g., via a crowdsourcing service).\n",
    "  - Also monitor your inputs’ quality (e.g., a malfunctioning sensor sending random values, or another team’s output becoming stale). This is particularly important for online learning systems.\n",
    "3. Retrain your models on a regular basis on fresh data (automate as much as possible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## TODO\n",
    "\n",
    "- Feature selection\n",
    "- Use cross validation for chosen model\n",
    "- Save figures back to filesystem\n",
    "\n",
    "### From Max:\n",
    "- extract month and year from the `transaction_date` column\n",
    "  - perform EDA on these new columns\n",
    "- look at zillow notebooks\n",
    "- check how RMSE is NOT insulated from outliers\n",
    "- check how MAE is insulated from outliers\n",
    "  - its not\n",
    "- type \n",
    "- if I had categorical independent variables, I shouldn't use the same high level analysis. Instead, i'd do different stuff, which we'll find in the future\n",
    "- use L1 to perform feature selection\n",
    "- use broken code to pick the top N, n-1, n-2, etc to 1 columns evaluate on TEST SET the \"best\", using MSE\n",
    "- validation set, no cross validation\n",
    "- neural network needs more hidden layers\n",
    "  - train on GPU, look into how difficult it is to setup\n",
    "  - actually pick a learning rate, start with 0.001\n",
    "  \n",
    "### From Scott:\n",
    "- ~~Elaborate that \"1. What are the current solutions/workarounds (if any)? None\" is just pretend~~\n",
    "- \"This may also sound really out there... but in choosing MAE ... totally fine with it... but wouldn't it be interesting to use Median instead of Mean? People default to MSE\"\n",
    "- ~~\"Would be useful to try and examine the ones that had some strange relationships from the massive grid corr plot\"~~\n",
    "  -~~ Basically do the grid first, then columns of interest after~~\n",
    "- bin lat/long\n",
    "  - could find \"beachfront\" for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Could Do\n",
    "\n",
    "- Add more normality tests\n",
    "- Attempt to figure out each column's distribution\n",
    "- Graph each data point, by `longitude` and `latitude` superimposed on a real map of Taiwan, with a heat map of the `house_price_of_unit_area`\n",
    "- use `Sklearn`'s `pipeline`s\n",
    "- use bootstrapping\n",
    "- reference [this](https://www.century21global.com/for-sale-residential/Taiwan/Taipei-City)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
